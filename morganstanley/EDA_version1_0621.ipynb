{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "from __future__ import print_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(filename):\n",
    "    text=pd.read_csv('filename',header=None,index_col=0)[1:]\n",
    "    temp=pd.DataFrame(text[1])\n",
    "    temp.dropna(inplace=True)\n",
    "    dict_=temp.to_dict()[1]\n",
    "    for key in dict_.keys():\n",
    "        dict_[key] = int(dict_[key])\n",
    "    \n",
    "    plt.figure(figsize=(20,20))\n",
    "    wordcloud = WordCloud(max_font_size=40, max_words=500,background_color=\"white\").generate_from_frequencies(dict_)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset():\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReviewsDataset, self).__init__(**kwargs)\n",
    "        # This will be initialised by the load method with all the dataset examples\n",
    "        self.X = None\n",
    "        self.X_original = None\n",
    "        # This will be initialised by the load method with all the dataset classes\n",
    "        self.y = None \n",
    "        \n",
    "#     def preprocess(self, remove_stopwords=False):\n",
    "#         # avoid tokenization: the input data are already tokenized\n",
    "#         tfidf = TfidfVectorizer(\n",
    "#             stop_words=\"english\" if remove_stopwords else None,\n",
    "#             min_df=2\n",
    "#         )\n",
    "        \n",
    "#         self.X = tfidf.fit_transform(self.X)\n",
    "        \n",
    "        \n",
    "    def load(self, filename,col='text'):\n",
    "        \"\"\"\n",
    "        Loads the dataset from the specified filename. \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Loading data from filename {}\".format(filename))\n",
    "        dataset=pd.read_csv(filename)\n",
    "        #need to specify column name, default is text\n",
    "        self.X = dataset[col].values\n",
    "        self.X_original= []        \n",
    "        self.y = dataset.sentiment\n",
    "        \n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"\n",
    "        Prints relevant information about the dataset \n",
    "        \"\"\"\n",
    "        # we assume that both X and Y have been correctly loaded\n",
    "        if self.X is None:\n",
    "            raise ValueError(\"Remember to call 'load' to load the dataset!\")\n",
    "        \n",
    "        print(\"Number of training examples %d\" % len(self.X))\n",
    "#         classname=[\"positive\", \"negative\"]\n",
    "#         classsize=Counter(self.y)\n",
    "#         print(classsize)\n",
    "        \n",
    "#         %matplotlib inline\n",
    "#         hist_names = [classname[t] for t in self.y]\n",
    "#         plt.subplot(4,1,4)\n",
    "#         plt.hist(hist_names)\n",
    "#         plt.show()\n",
    "        \n",
    "\n",
    "        vocabulary = Counter()\n",
    "        sum_of_words = 0\n",
    "        \n",
    "        for words in self.X:\n",
    "            sum_of_words += len(words)\n",
    "            for word in words:\n",
    "                 vocabulary.update([word])\n",
    "        print(\"Average sentence length: {} words\".format(  \n",
    "              str(sum_of_words / float(len(self.X)))))\n",
    "        print(\"Vobcaulary size: \" + str(len(vocabulary)))\n",
    "        print(vocabulary.most_common(10))\n",
    "        print(vocabulary.most_common()[:-11:-1])\n",
    "        \n",
    "        fdist1= FreqDist(vocabulary)\n",
    "        for key in fdist1:\n",
    "            fdist1[key]=math.log(fdist1[key])\n",
    "        %matplotlib inline\n",
    "        plt.subplot(4,1,1)\n",
    "        fdist1.plot(50)\n",
    "       \n",
    "        frequence_ratio=[]\n",
    "        log_rank=[]\n",
    "        log_frequence=[]\n",
    "        vocab1 = sorted(vocabulary.items(), key=lambda d:d[1], reverse = True)\n",
    "        vocab1=np.array(vocab1)\n",
    "        for index in range(0,vocab1.shape[0]):\n",
    "            log_rank.append(math.log(index+1))\n",
    "            frequence_ratio.append(vocab1[index,1]*(index+1))\n",
    "            log_frequence.append(math.log(vocab1[index,1]))\n",
    "        print(\"the mean of frequence ratio\":dnp.mean(frequence_ratio))\n",
    "        print(\"the std of frequence ratio\":np.std(frequence_ratio))\n",
    "        %matplotlib inline\n",
    "        plt.subplot(4,1,2)\n",
    "        plt.plot(np.array(log_rank),np.array(log_frequence))\n",
    "        plt.title('log(rank) vs log(frequence)')\n",
    "        plt.xlabel('log(rank)')\n",
    "        plt.ylabel('log(frequence)')\n",
    "        plt.show()\n",
    "        %matplotlib inline\n",
    "        plt.subplot(4,1,3)\n",
    "        plt.plot(frequence_ratio)\n",
    "        plt.title('Plot of frequence ratio')\n",
    "        plt.show()\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))\n",
    "    \n",
    "# Storing the entire training text in a list\n",
    "text = list(train.text.values)\n",
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try lda\n",
    "n_components=11\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "n_top_words = 40\n",
    "print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word cloud for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud_sub(num,tf_feature_names=tf_feature_names):\n",
    "    topic=lda.components_[num]\n",
    "    topic_words = [tf_feature_names[i] for i in topic.argsort()[:-50 - 1 :-1]]\n",
    "    cloud = WordCloud(\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(topic_words))\n",
    "    plt.imshow(cloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(n_components):\n",
    "    word_cloud_sub(num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
